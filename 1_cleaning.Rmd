---
title: "Cleaning template general"
author: "Your Name"
date: "January 2024"
output: utilityR::cleaning_template
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r setup the values}
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
rm(list = ls())


directory_dictionary <- list(
  research_cycle_name = 'xxxx',
  round = 'xxxx',
  dir.audits = "data/inputs/audits/reach/", # The directory to your audit files
  dir.audits.check = "output/checking/audit/",# The directory to your audit summary files (you'll be checking these)
  dir.requests = "output/checking/requests/", # the directory of your other_requests file
  dir.responses = "output/checking/responses/", # the directory of your responses to open questions
  enum_colname = "XXX", # the column that contains the enumerator ID,
  enum_comments = 'XXX', # the column that contains the enumerator's comments,
  filename.tool = "resources/XXX.xlsx", # the name of your Kobo tool and its path
  data_name = "XXXX.xlsx", # the name of your dataframe
  data_path = "data/inputs/kobo_export/", # the path to your dataframe
  label_colname = 'label::English', # the name of your label column. Has to be identical in Kobo survey and choices sheets
  dctime_short = "XXXX" # the data of your survey (just for naming)
)

```

Initialize packages, load tools.
Add the API key file into `resources` directory prior to running this

```{r initialize_packages_load_inputs}

api_key <- source('resources/microsoft.api.key_regional.R')$value

#-------------------------------Initialize packages, load tools -----------------------------
source("src/init.R")
source("src/load_Data.R")

```

Section below only for research cycles that requires cleaning on regular basis and use one kobo server.

```{r Section  0  - Data pre-processing}

source('src/sections/process_old_data.R')


# final preparation
# Rename your dataframes

raw.main <- kobo.raw.main

sheet_names <- sheet_names[sheet_names!='kobo.raw.main']
sheet_names_new <- gsub('kobo.','',sheet_names)

if(length(sheet_names_new)>0){
  for(i in 1:length(sheet_names_new)){
    txt <- paste0(sheet_names_new[i],' <- ',sheet_names[i])
    eval(parse(text=txt))
  }
}


# If there were any changes in the tool during data collection, they can be run here
source('src/sections/tool_modification.R')


# select the columns in your data that contain date elements
date_cols_main <- c("start","end", tool.survey %>% filter(type == "date" & datasheet == "main") %>% pull(name),
                    "submission_time") # add them here

# transform them into the datetime format
raw.main <- raw.main %>%
  mutate_at(date_cols_main, ~ifelse(!str_detect(., '-'), as.character(convertToDateTime(as.numeric(.))), .))

rm(date_cols_main)


```

Go into the script to specify the no-consent codition. Otherwise no manual entry is necessary on this bit of the script

```{r Section  1  - Remove duplicates and No consent entries}
source('src/sections/section_1_remove_duplicates_no_consents.R')
```

Specify the necessary parameters of the audit check in the rows below and run the source command

```{r Section  2  - Audit checks + soft duplicates}
min_duration_interview <- 5 # minimum duration of an interview (screen time in minutes)
max_duration_interview <- 60 # maximum duration of an interview (screen time in minutes)
pre_process_audit_files <- F # whether cases of respondent taking too long to answer 1 question should cleaned.
# if pre_process_audit_files =T, enter the maximum time that  the respondent can spend answering 1 question (in minutes)
max_length_answer_1_question <- 20
# Used during the check for soft duplicates.
# The minimum number of different columns that makes us confident that the entry is not a soft duplicate
min_num_diff_questions <- 8

# run the checks
source('src/sections/section_2_run_audit_checks.R')
```

Once you've checked all entries in the files in "output/checking/audit/" directory and only left those rows that need to be deleted implement the decisions below

```{r Section  2 - Implement deletion decisions}
source('src/sections/section_2_run_audit_decisions.R')
```

The section below is deprecated and rarely used. We're keeping it in case it comes in handy sometime in the future

```{r Section  3  - Loop inconsitencies + spatial checks}
#specify the column that holds the cordinates
geo_column <- ''

source('src/sections/section_3_loops_and_spatial_checks.R')


```
Once you've checked all entries in the "output/checking/audit/geospatial_check"  and only left what needs to be deleted implement the decisions below
```{r Section  3  - Loop inconsitencies + spatial checks decisions}
source('src/sections/section_3_spatial_decisions.R')

# duplicates are merged only prior to writing the file.
deletion.log.new <- bind_rows(deletion.log.dupl,deletion.log.new)
write.xlsx(deletion.log.new, make.filename.xlsx("output/deletion_log/", "deletion_log", no_date = T), overwrite=T)

```


Section below is a bit more involved. The first section can be run without your direct input, just be aware that this section will activate the translation function.

```{r Section  4 - Others and translations - setup}
source('src/sections/section_4_create_other_requests_files.R')
```

After receiving the outputs of this function, please check the `dir.requests` directory and work through the other requests files filling them out. 

The section below is better run if you open up the file and run the script line by line.

```{r Section  4 - Others and translations - recoding}
# name that hosts the clean recode.others file, leave as '' if you don't have this file. Nothing will be recoded that way
name_clean_others_file <- 'XXX'
sheet_name_others <- 'Sheet2' # name of the sheet where you're holding your requests
# name that hosts the clean translation requests file, leave as '' if you don't have this file. Nothing will be recoded that way
name_clean_trans_file <- 'XXX'

source('src/sections/section_4_apply_changes_to_requests.R')

```

Some post translation checks to make sure it all worked out. Doesn't need much involvement from your side if you did everything well. If not, you will get some warnings.

```{r  Section  4 - additional checks}

# Check if your data still has any non-english entries

# variables that will be omitted from the analysis
vars_to_omit <- c('settlement', directory_dictionary$enum_colname, directory_dictionary$enum_comments) # add more names as needed

source('src/sections/section_4_post_check_for_leftover_non_eng.R')

# Check that cumulative and binary values in select multiple match each other

cleaning.log.match <- utilityR::select.multiple.check(raw.main, tool.survey, id_col="uuid")

if (nrow(cleaning.log.match) > 0) {
  write.xlsx(cleaning.log.match, "output/checking/select_multiple_match.xlsx", overwrite=T)
}
```

Check if any columns are equal to '999'/'99', enter any other values you're suspicious of

```{r Section  5 - Check for 999/99 entries}
code_for_check  <- c('99','999')

source('src/sections/section_5_create_999_checks.R')

print(cl_log_999)
```
if Anything got into cl_log_999, check it. If you want to delete it from your data run the command below set apply_999_changes to 'Yes' if you want to remove the entries from code_for_check

```{r Section  5 - Fix 999/99 entries}
apply_999_changes <- 'No'
cl_log_999 <- readxl::read_excel("output/checking/999_diferences.xlsx") %>% select(-!!sym(directory_dictionary$enum_colname))
source('src/sections/section_5_finish_999_checks.R')
```

Time for logic checks
```{r Section L - Your logic checks go here}
cleaning.log.checks.direct <- tibble()
```

Outlier checks will require you to set some parameters prior to running the algorithm. Check the documentation on `detect.outliers` to find more details.
```{Section 6 - Check for outliers}
# specify the number of standard deviations you want to use
n.sd <- 3

# specify methods for  detecting outliers
method <- "o1"

# ignore 0 values or not
ignore_0 <- T

# specify as many loops as you need 
# specify columns for check or leave them empty
cols.integer_raw.main <- c()
cols.integer_raw.loop1 <- c()
cols.integer_raw.loop2 <- c()
cols.integer_raw.loop3 <- c()

source('src/sections/section_6_detect_and_visualise_outliers.R')


```

Edit the file
Manually check outliers and change to NA (Decision made with country FPS)
Save your changes in the same file or create a new file in output/checking/responses/ and change the path in the `cleaning.log.outliers` below

```{r}
cleaning.log.outliers <- read.xlsx(paste0("output/checking/outliers/outlier_analysis_", n.sd, "sd.xlsx"),
                                   sheet = 1)

cleaning.log.outliers <- cleaning.log.outliers %>% filter(checked%==%'value corrected')

source('src/sections/section_6_finish_outlier_check.R')

cleaning.log <- bind_rows(cleaning.log,cleaning.log.outliers)

```

You're done with cleaning (yay). Now you have to drop the unnecessary data and fill in the template for HQ valiadation.

```{r Section 7 - Finalize the cleaning log}

# finalize cleaning log:
cleaning.log <- cleaning.log %>% distinct() %>%
  filter(old.value %not=na% new.value) %>% left_join(raw.main %>% select(uuid, any_of(directory_dictionary$enum_colname)))

if (length(list.files(make.filename.xlsx("output/cleaning_log", "cleaning_log", no_date = T))) > 0) {
  cleaning.log.previous <- read_xlsx(make.filename.xlsx("output/cleaning_log", "cleaning_log"))
  cleaning.log.whole <- rbind(cleaning.log.previous, cleaning.log)
} else {
  cleaning.log.whole <- cleaning.log
}
# Output Cleaning Log
write.xlsx(cleaning.log.whole, make.filename.xlsx("output/cleaning_log", "cleaning_log", no_date = T), overwrite = T)

```


```{r Section 7 Remove PII columns}
# specify the PII columns to remove
pii.to.remove_main <- c(
  "deviceid",
  "staff_other",
  "audit",
  "audit_URL",
  "username")

# remove PII from raw.
raw.main  <- raw.main %>% select(-any_of(pii.to.remove_main))
kobo.raw.main  <- kobo.raw.main %>% select(-any_of(pii.to.remove_main))

if(length(ls)>1){
  ls_loops <- ls[2:length(ls)]
}else{ls_loops <- c()}
```


```{r Section 7 - Save the raw dataset with no PII}
# create the data list that will be written into the excel 
if(length(ls)>1){
txt <- paste0(
  'datasheets <-list("main" =kobo.raw.main,',
  paste0('"',ls_loops,'" = ',sheet_names, collapse = ','),')'
)
}else{
  txt <- 'datasheets <-list("main" =kobo.raw.main)'
}
eval(parse(text= txt))

write.xlsx(datasheets, make.filename.xlsx("output/data_log", "full_data"), overwrite = T,
           zoom = 90, firstRow = T)
```

```{r Section 7 - Save the clean dataset with no PII}
# clean data (pii removed)

if(length(ls)>1){
txt <- paste0(
  'datasheets_anon <-list("main" =raw.main,',
  paste0('"',ls_loops,'" = ',sheet_names_new, collapse = ','),')'
)}else{
  txt <- 'datasheets_anon <-list("main" =raw.main)'
}
eval(parse(text= txt))

write.xlsx(datasheets_anon, make.filename.xlsx("output/final", "final_anonymized_data"), overwrite = T,
           zoom = 90, firstRow = T)
```

```{r Section 7 - Run the enumerator performance script and zip the files into one archive}
source("src/Cleaning_logbook.R")
source("package4validation.R")
```

You are done. Congratulations :)
